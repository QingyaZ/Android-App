{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9kuBTmZPLnB"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/yandex-research/rtdl-revisiting-models/blob/main/package/example.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "---\n",
        "\n",
        "**See also** [RTDL](https://github.com/yandex-research/rtdl)\n",
        "-- **other projects on tabular deep learning**.\n",
        "\n",
        "---\n",
        "\n",
        "- This notebook provides a usage example of the\n",
        "  [rtdl_revisiting_models](https://github.com/yandex-research/rtdl-revisiting-models)\n",
        "  package.\n",
        "- Hyperparameters are not tuned and may be suboptimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsIIj9u0PLnD",
        "outputId": "e1999bd4-8fcb-4ba8-b420-bc224712099a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting delu==0.0.23\n",
            "  Downloading delu-0.0.23-py3-none-any.whl.metadata (805 bytes)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.11/dist-packages (from delu==0.0.23) (1.26.4)\n",
            "Requirement already satisfied: torch<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from delu==0.0.23) (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->delu==0.0.23) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->delu==0.0.23) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->delu==0.0.23) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->delu==0.0.23) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->delu==0.0.23) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=1.8->delu==0.0.23)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=1.8->delu==0.0.23)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=1.8->delu==0.0.23)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=1.8->delu==0.0.23)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=1.8->delu==0.0.23)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=1.8->delu==0.0.23)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=1.8->delu==0.0.23)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=1.8->delu==0.0.23)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=1.8->delu==0.0.23)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->delu==0.0.23) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->delu==0.0.23) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=1.8->delu==0.0.23)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->delu==0.0.23) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->delu==0.0.23) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=1.8->delu==0.0.23) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=1.8->delu==0.0.23) (3.0.2)\n",
            "Downloading delu-0.0.23-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m562.5/664.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install delu==0.0.23\n",
        "%pip install rtdl_revisiting_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRFzdozyPLnD"
      },
      "outputs": [],
      "source": [
        "# ruff: noqa: E402\n",
        "import math\n",
        "import warnings\n",
        "from typing import Dict, Literal\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import delu  # Deep Learning Utilities: https://github.com/Yura52/delu\n",
        "import numpy as np\n",
        "import scipy.special\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "from torch import Tensor\n",
        "from tqdm.std import tqdm\n",
        "\n",
        "warnings.resetwarnings()\n",
        "\n",
        "from rtdl_revisiting_models import MLP, ResNet, FTTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn6NIkjnPLnE"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Set random seeds in all libraries.\n",
        "delu.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1NeFsR_PLnE"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJWH3SpbPLnE"
      },
      "outputs": [],
      "source": [
        "# >>> Dataset.\n",
        "TaskType = Literal[\"regression\", \"binclass\", \"multiclass\"]\n",
        "\n",
        "task_type: TaskType = \"regression\"\n",
        "n_classes = None\n",
        "dataset = sklearn.datasets.fetch_california_housing()\n",
        "X_cont: np.ndarray = dataset[\"data\"]\n",
        "Y: np.ndarray = dataset[\"target\"]\n",
        "\n",
        "# NOTE: uncomment to solve a classification task.\n",
        "# n_classes = 2\n",
        "# assert n_classes >= 2\n",
        "# task_type: TaskType = 'binclass' if n_classes == 2 else 'multiclass'\n",
        "# X_cont, Y = sklearn.datasets.make_classification(\n",
        "#     n_samples=20000,\n",
        "#     n_features=8,\n",
        "#     n_classes=n_classes,\n",
        "#     n_informative=3,\n",
        "#     n_redundant=2,\n",
        "# )\n",
        "\n",
        "# >>> Continuous features.\n",
        "X_cont: np.ndarray = X_cont.astype(np.float32)\n",
        "n_cont_features = X_cont.shape[1]\n",
        "\n",
        "# >>> Categorical features.\n",
        "# NOTE: the above datasets do not have categorical features, but,\n",
        "# for the demonstration purposes, it is possible to generate them.\n",
        "cat_cardinalities = [\n",
        "    # NOTE: uncomment the two lines below to add two categorical features.\n",
        "    # 4,  # Allowed values: [0, 1, 2, 3].\n",
        "    # 7,  # Allowed values: [0, 1, 2, 3, 4, 5, 6].\n",
        "]\n",
        "X_cat = (\n",
        "    np.column_stack(\n",
        "        [np.random.randint(0, c, (len(X_cont),)) for c in cat_cardinalities]\n",
        "    )\n",
        "    if cat_cardinalities\n",
        "    else None\n",
        ")\n",
        "\n",
        "# >>> Labels.\n",
        "# Regression labels must be represented by float32.\n",
        "if task_type == \"regression\":\n",
        "    Y = Y.astype(np.float32)\n",
        "else:\n",
        "    assert n_classes is not None\n",
        "    Y = Y.astype(np.int64)\n",
        "    assert set(Y.tolist()) == set(\n",
        "        range(n_classes)\n",
        "    ), \"Classification labels must form the range [0, 1, ..., n_classes - 1]\"\n",
        "\n",
        "# >>> Split the dataset.\n",
        "all_idx = np.arange(len(Y))\n",
        "trainval_idx, test_idx = sklearn.model_selection.train_test_split(\n",
        "    all_idx, train_size=0.8\n",
        ")\n",
        "train_idx, val_idx = sklearn.model_selection.train_test_split(\n",
        "    trainval_idx, train_size=0.8\n",
        ")\n",
        "data_numpy = {\n",
        "    \"train\": {\"x_cont\": X_cont[train_idx], \"y\": Y[train_idx]},\n",
        "    \"val\": {\"x_cont\": X_cont[val_idx], \"y\": Y[val_idx]},\n",
        "    \"test\": {\"x_cont\": X_cont[test_idx], \"y\": Y[test_idx]},\n",
        "}\n",
        "if X_cat is not None:\n",
        "    data_numpy[\"train\"][\"x_cat\"] = X_cat[train_idx]\n",
        "    data_numpy[\"val\"][\"x_cat\"] = X_cat[val_idx]\n",
        "    data_numpy[\"test\"][\"x_cat\"] = X_cat[test_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ2GSlxCPLnE"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9hN-IdzPLnF"
      },
      "outputs": [],
      "source": [
        "# >>> Feature preprocessing.\n",
        "# NOTE\n",
        "# The choice between preprocessing strategies depends on a task and a model.\n",
        "\n",
        "# (A) Simple preprocessing strategy.\n",
        "# preprocessing = sklearn.preprocessing.StandardScaler().fit(\n",
        "#     data_numpy['train']['x_cont']\n",
        "# )\n",
        "\n",
        "# (B) Fancy preprocessing strategy.\n",
        "# The noise is added to improve the output of QuantileTransformer in some cases.\n",
        "X_cont_train_numpy = data_numpy[\"train\"][\"x_cont\"]\n",
        "noise = (\n",
        "    np.random.default_rng(0)\n",
        "    .normal(0.0, 1e-5, X_cont_train_numpy.shape)\n",
        "    .astype(X_cont_train_numpy.dtype)\n",
        ")\n",
        "preprocessing = sklearn.preprocessing.QuantileTransformer(\n",
        "    n_quantiles=max(min(len(train_idx) // 30, 1000), 10),\n",
        "    output_distribution=\"normal\",\n",
        "    subsample=10**9,\n",
        ").fit(X_cont_train_numpy + noise)\n",
        "del X_cont_train_numpy\n",
        "\n",
        "for part in data_numpy:\n",
        "    data_numpy[part][\"x_cont\"] = preprocessing.transform(data_numpy[part][\"x_cont\"])\n",
        "\n",
        "# >>> Label preprocessing.\n",
        "if task_type == \"regression\":\n",
        "    Y_mean = data_numpy[\"train\"][\"y\"].mean().item()\n",
        "    Y_std = data_numpy[\"train\"][\"y\"].std().item()\n",
        "    for part in data_numpy:\n",
        "        data_numpy[part][\"y\"] = (data_numpy[part][\"y\"] - Y_mean) / Y_std\n",
        "\n",
        "# >>> Convert data to tensors.\n",
        "data = {\n",
        "    part: {k: torch.as_tensor(v, device=device) for k, v in data_numpy[part].items()}\n",
        "    for part in data_numpy\n",
        "}\n",
        "\n",
        "if task_type != \"multiclass\":\n",
        "    # Required by F.binary_cross_entropy_with_logits\n",
        "    for part in data:\n",
        "        data[part][\"y\"] = data[part][\"y\"].float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYuVW1qvPLnF"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcAhDuI1PLnF"
      },
      "outputs": [],
      "source": [
        "# The output size.\n",
        "d_out = n_classes if task_type == \"multiclass\" else 1\n",
        "\n",
        "# # NOTE: uncomment to train MLP\n",
        "# model = MLP(\n",
        "#     d_in=n_cont_features + sum(cat_cardinalities),\n",
        "#     d_out=d_out,\n",
        "#     n_blocks=2,\n",
        "#     d_block=384,\n",
        "#     dropout=0.1,\n",
        "# ).to(device)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "\n",
        "# # NOTE: uncomment to train ResNet\n",
        "# model = ResNet(\n",
        "#     d_in=n_cont_features + sum(cat_cardinalities),\n",
        "#     d_out=d_out,\n",
        "#     n_blocks=2,\n",
        "#     d_block=192,\n",
        "#     d_hidden=None,\n",
        "#     d_hidden_multiplier=2.0,\n",
        "#     dropout1=0.3,\n",
        "#     dropout2=0.0,\n",
        "# ).to(device)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "\n",
        "model = FTTransformer(\n",
        "    n_cont_features=n_cont_features,\n",
        "    cat_cardinalities=cat_cardinalities,\n",
        "    d_out=d_out,\n",
        "    **FTTransformer.get_default_kwargs(),\n",
        ").to(device)\n",
        "optimizer = model.make_default_optimizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-phdRlcPLnF"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXZk8MsGPLnF"
      },
      "outputs": [],
      "source": [
        "def apply_model(batch: Dict[str, Tensor]) -> Tensor:\n",
        "    if isinstance(model, (MLP, ResNet)):\n",
        "        x_cat_ohe = (\n",
        "            [\n",
        "                F.one_hot(column, cardinality)\n",
        "                for column, cardinality in zip(batch[\"x_cat\"].T, cat_cardinalities)\n",
        "            ]\n",
        "            if \"x_cat\" in batch\n",
        "            else []\n",
        "        )\n",
        "        return model(torch.column_stack([batch[\"x_cont\"]] + x_cat_ohe)).squeeze(-1)\n",
        "\n",
        "    elif isinstance(model, FTTransformer):\n",
        "        return model(batch[\"x_cont\"], batch.get(\"x_cat\")).squeeze(-1)\n",
        "\n",
        "    else:\n",
        "        raise RuntimeError(f\"Unknown model type: {type(model)}\")\n",
        "\n",
        "\n",
        "loss_fn = (\n",
        "    F.binary_cross_entropy_with_logits\n",
        "    if task_type == \"binclass\"\n",
        "    else F.cross_entropy\n",
        "    if task_type == \"multiclass\"\n",
        "    else F.mse_loss\n",
        ")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(part: str) -> float:\n",
        "    model.eval()\n",
        "\n",
        "    eval_batch_size = 8096\n",
        "    y_pred = (\n",
        "        torch.cat(\n",
        "            [\n",
        "                apply_model(batch)\n",
        "                for batch in delu.iter_batches(data[part], eval_batch_size)\n",
        "            ]\n",
        "        )\n",
        "        .cpu()\n",
        "        .numpy()\n",
        "    )\n",
        "    y_true = data[part][\"y\"].cpu().numpy()\n",
        "\n",
        "    if task_type == \"binclass\":\n",
        "        y_pred = np.round(scipy.special.expit(y_pred))\n",
        "        score = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "    elif task_type == \"multiclass\":\n",
        "        y_pred = y_pred.argmax(1)\n",
        "        score = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "    else:\n",
        "        assert task_type == \"regression\"\n",
        "        score = -(sklearn.metrics.mean_squared_error(y_true, y_pred) ** 0.5 * Y_std)\n",
        "    return score  # The higher -- the better.\n",
        "\n",
        "\n",
        "print(f'Test score before training: {evaluate(\"test\"):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIZUjDgqPLnF"
      },
      "outputs": [],
      "source": [
        "# For demonstration purposes (fast training and bad performance),\n",
        "# one can set smaller values:\n",
        "# n_epochs = 20\n",
        "# patience = 2\n",
        "n_epochs = 1_000_000_000\n",
        "patience = 16\n",
        "\n",
        "batch_size = 256\n",
        "epoch_size = math.ceil(len(train_idx) / batch_size)\n",
        "timer = delu.tools.Timer()\n",
        "early_stopping = delu.tools.EarlyStopping(patience, mode=\"max\")\n",
        "best = {\n",
        "    \"val\": -math.inf,\n",
        "    \"test\": -math.inf,\n",
        "    \"epoch\": -1,\n",
        "}\n",
        "\n",
        "print(f\"Device: {device.type.upper()}\")\n",
        "print(\"-\" * 88 + \"\\n\")\n",
        "timer.run()\n",
        "for epoch in range(n_epochs):\n",
        "    for batch in tqdm(\n",
        "        delu.iter_batches(data[\"train\"], batch_size, shuffle=True),\n",
        "        desc=f\"Epoch {epoch}\",\n",
        "        total=epoch_size,\n",
        "    ):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(apply_model(batch), batch[\"y\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    val_score = evaluate(\"val\")\n",
        "    test_score = evaluate(\"test\")\n",
        "    print(f\"(val) {val_score:.4f} (test) {test_score:.4f} [time] {timer}\")\n",
        "\n",
        "    early_stopping.update(val_score)\n",
        "    if early_stopping.should_stop():\n",
        "        break\n",
        "\n",
        "    if val_score > best[\"val\"]:\n",
        "        print(\"ðŸŒ¸ New best epoch! ðŸŒ¸\")\n",
        "        best = {\"val\": val_score, \"test\": test_score, \"epoch\": epoch}\n",
        "    print()\n",
        "\n",
        "print(\"\\n\\nResult:\")\n",
        "print(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnkhHDPfPLnG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}